{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Thai Grapheme Classification Algorithm\n",
    "\n",
    "This algorithm classifies Thai graphemes into **three main classes**, with subclasses, plus a special case for **อ**.\n",
    "\n",
    "1. **ฐาน (tan)** - *foundation class*\n",
    "   Consonant letters that act as the **base** for dependent marks (vowels, tone marks, etc.).\n",
    "\n",
    "2. **สระ (sara)** - *vowel class*\n",
    "   Vowel graphemes (both independent and dependent), which attach to a foundation consonant.\n",
    "\n",
    "3. **ยุกต์ (yuk)** - *dependent class*\n",
    "   Tone marks and other diacritics that cannot exist without a foundation consonant.\n",
    "\n",
    "4. **ข้อยกเว้น (kho yok waen)** - *exception class*\n",
    "   The consonant **อ** is treated separately, since it functions both as a **foundation** (carrier consonant) and as part of certain **vowel symbols**.\n",
    "\n"
   ],
   "id": "d244a6a94349f29e"
  },
  {
   "cell_type": "code",
   "id": "37s83qydue3",
   "source": "# Test cases\nthai1 = \"ยา\"  # simple: x=ย, vowel=า\nthai2 = \"เด็ก\"  # pattern เx็f: x=ด, vowel=เ็, f=ก\nthai3 = \"คน\"  # x=ค, f=น (hidden vowel)\nthai4 = \"เลว\"  # AMBIGUOUS: could be เxf (x=ล, f=ว) OR cluster ลว + เx\n\nthaihard1 = \"เกรียน\"  # pattern เxียf: x=กร (cluster), vowel=เีย, f=น\nthaihard2 = \"เอา\"  # pattern เxา: x=อ (silent), vowel=เา\nthaihard3 = \"อย่า\"  # x=อ (silent), vowel=่า\nthaihard4 = \"เอือม\"  # pattern เxือf: x=อ, vowel=เือ, f=ม\nthaihard5 = \"ไกล\"  # pattern ไxf: x=ก, vowel=ไ, f=ล",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:14:54.531187Z",
     "start_time": "2025-09-21T20:14:54.528145Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "wifxxzx17x",
   "source": [
    "import json\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "# Load data\n",
    "with open(\"../../res/foundation/foundation.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data_foundation = json.load(f)\n",
    "\n",
    "with open(\"../../thai_vowels_tagged_9-21-2025-2-31-pm.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tagged_vowel_data = json.load(f)\n",
    "\n",
    "foundation = set(data_foundation[\"foundation\"])\n",
    "vowel_patterns = list(tagged_vowel_data[\"patterns\"].keys())\n",
    "\n",
    "print(f\"Loaded {len(foundation)} foundation consonants\")\n",
    "print(f\"Loaded {len(vowel_patterns)} vowel patterns\")\n",
    "\n",
    "# Characters that can be ambiguous (can be part of cluster OR part of vowel)\n",
    "ambiguous_chars = {\"ว\", \"ย\", \"อ\"}  # ว and ย can be final consonants OR part of vowels; อ can be initial OR part of vowel"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:14:54.554673Z",
     "start_time": "2025-09-21T20:14:54.550986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 42 foundation consonants\n",
      "Loaded 72 vowel patterns\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "n9ngadrxa9c",
   "source": "def find_all_reading_orders(text: str) -> List[Dict]:\n    \"\"\"\n    Find all possible reading orders for Thai text based on vowel patterns.\n    Returns a list of possible interpretations.\n    \n    Each interpretation contains:\n    - segments: List of (grapheme, reading_order, role) tuples\n    - ambiguity: Whether this reading has ambiguous elements\n    - ambiguity_reason: Why it's ambiguous (if applicable)\n    \"\"\"\n    if not text:\n        return []\n    \n    interpretations = []\n    \n    # Try to match vowel patterns at each position\n    def try_patterns(pos: int, consumed: Set[int], current_segments: List):\n        if len(consumed) == len(text):\n            # We've consumed all characters - this is a valid interpretation\n            interpretations.append({\n                'segments': sorted(current_segments, key=lambda x: x[1]),\n                'ambiguity': False,\n                'ambiguity_reason': None\n            })\n            return\n        \n        # Skip already consumed positions\n        if pos in consumed:\n            try_patterns(pos + 1, consumed, current_segments)\n            return\n        \n        if pos >= len(text):\n            # We have unconsumed characters - add them as standalone\n            for i in range(len(text)):\n                if i not in consumed:\n                    char = text[i]\n                    role = 'unmatched'\n                    current_segments.append((char, i, role))\n                    consumed.add(i)\n            interpretations.append({\n                'segments': sorted(current_segments, key=lambda x: x[1]),\n                'ambiguity': True,\n                'ambiguity_reason': 'unmatched characters'\n            })\n            return\n        \n        # Try each vowel pattern starting at this position\n        matched_any = False\n        for pattern in vowel_patterns:\n            match_result = try_match_pattern(text, pos, pattern, consumed)\n            if match_result:\n                matched_text, positions, x_pos, f_pos = match_result\n                new_consumed = consumed.copy()\n                new_segments = current_segments.copy()\n                \n                # Add segments with reading order\n                reading_order = 0\n                \n                # Initial consonant(s) always read first\n                if x_pos:\n                    x_text = ''.join(text[p] for p in x_pos)\n                    new_segments.append((x_text, reading_order, 'initial'))\n                    reading_order += 1\n                    new_consumed.update(x_pos)\n                \n                # Vowel parts read second\n                vowel_positions = [p for p in positions if p not in x_pos and (not f_pos or p not in f_pos)]\n                if vowel_positions:\n                    vowel_text = pattern  # Store the pattern as the vowel identifier\n                    new_segments.append((vowel_text, reading_order, 'vowel_pattern'))\n                    reading_order += 1\n                    new_consumed.update(vowel_positions)\n                \n                # Final consonant read last\n                if f_pos:\n                    f_text = ''.join(text[p] for p in f_pos)\n                    new_segments.append((f_text, reading_order, 'final'))\n                    new_consumed.update(f_pos)\n                \n                # Continue matching from next unconsumed position\n                next_pos = 0\n                while next_pos < len(text) and next_pos in new_consumed:\n                    next_pos += 1\n                \n                try_patterns(next_pos, new_consumed, new_segments)\n                matched_any = True\n        \n        # If no pattern matched, try treating current character as standalone\n        if not matched_any:\n            if pos not in consumed:\n                new_consumed = consumed.copy()\n                new_segments = current_segments.copy()\n                new_consumed.add(pos)\n                new_segments.append((text[pos], pos, 'standalone'))\n                try_patterns(pos + 1, new_consumed, new_segments)\n    \n    try_patterns(0, set(), [])\n    \n    # Identify ambiguities\n    for interp in interpretations:\n        # Check for ว ambiguity\n        for seg in interp['segments']:\n            if 'ว' in seg[0] and (seg[2] == 'final' or seg[2] in ['initial', 'cluster']):\n                interp['ambiguity'] = True\n                interp['ambiguity_reason'] = 'ว could be part of cluster or vowel pattern'\n                break\n    \n    return interpretations\n\ndef try_match_pattern(text: str, start_pos: int, pattern: str, consumed: Set[int]) -> Tuple:\n    \"\"\"\n    Try to match a vowel pattern at the given position.\n    Returns: (matched_text, all_positions, x_positions, f_positions) or None\n    \"\"\"\n    positions = []\n    x_positions = []\n    f_positions = []\n    text_idx = start_pos\n    pattern_idx = 0\n    \n    while pattern_idx < len(pattern) and text_idx < len(text):\n        if text_idx in consumed:\n            return None\n            \n        pattern_char = pattern[pattern_idx]\n        \n        if pattern_char == 'x':\n            # x = initial consonant(s) - can be cluster\n            if text[text_idx] in foundation:\n                x_positions.append(text_idx)\n                positions.append(text_idx)\n                text_idx += 1\n                \n                # Check for possible cluster\n                while text_idx < len(text) and text[text_idx] in foundation and text_idx not in consumed:\n                    # Look ahead to see if this could be part of cluster\n                    if pattern_idx + 1 < len(pattern) and pattern[pattern_idx + 1] != 'f':\n                        # Could be cluster, but we'll explore both options\n                        break\n                    text_idx += 1\n                    \n            else:\n                return None\n                \n        elif pattern_char == 'f':\n            # f = final consonant\n            if text[text_idx] in foundation:\n                f_positions.append(text_idx)\n                positions.append(text_idx)\n                text_idx += 1\n            else:\n                return None\n                \n        else:\n            # Must match exact character\n            if text[text_idx] == pattern_char:\n                positions.append(text_idx)\n                text_idx += 1\n            else:\n                return None\n        \n        pattern_idx += 1\n    \n    # Check if pattern fully matched\n    if pattern_idx == len(pattern):\n        return (''.join(text[p] for p in positions), positions, x_positions, f_positions)\n    \n    return None",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:14:54.566071Z",
     "start_time": "2025-09-21T20:14:54.559040Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "yqikrw0l2s9",
   "source": "def find_all_possible_readings(text: str) -> List[List[Tuple]]:\n    \"\"\"\n    Find ALL possible reading orders for Thai text by trying every valid pattern match.\n    \n    Key insight: We need to find ALL ways to segment the text into syllables,\n    where each syllable has one vowel pattern.\n    \n    Returns: List of possible readings, where each reading is a list of \n             (initial_consonant(s), vowel_pattern, final_consonant) tuples\n    \"\"\"\n    if not text:\n        return []\n    \n    all_readings = []\n    text_len = len(text)\n    \n    def find_pattern_matches_at_position(text: str, pattern: str) -> List[Dict]:\n        \"\"\"\n        Find all ways this pattern could match in the text.\n        Returns list of matches with their positions and components.\n        \"\"\"\n        matches = []\n        \n        # For patterns with 'x', we need to try different cluster sizes\n        has_x = 'x' in pattern\n        has_f = 'f' in pattern\n        \n        # Try matching at each position\n        for start_pos in range(len(text)):\n            if has_x:\n                # Try different cluster sizes (1-3 consonants typically)\n                for cluster_size in range(1, min(4, len(text) - start_pos + 1)):\n                    match = try_match_with_specific_cluster_size(\n                        text, start_pos, pattern, cluster_size\n                    )\n                    if match:\n                        matches.append(match)\n            else:\n                # No x in pattern, just try direct match\n                match = try_match_with_specific_cluster_size(\n                    text, start_pos, pattern, 0\n                )\n                if match:\n                    matches.append(match)\n        \n        return matches\n    \n    def try_match_with_specific_cluster_size(text: str, start_pos: int, \n                                            pattern: str, cluster_size: int) -> Dict:\n        \"\"\"\n        Try to match pattern at start_pos with specific cluster size for 'x'.\n        Returns match info or None.\n        \"\"\"\n        text_idx = start_pos\n        pattern_idx = 0\n        positions_used = []\n        x_text = \"\"\n        f_text = \"\"\n        \n        while pattern_idx < len(pattern) and text_idx < len(text):\n            p_char = pattern[pattern_idx]\n            \n            if p_char == 'x':\n                # Match cluster_size consonants\n                consumed = 0\n                x_chars = []\n                while consumed < cluster_size and text_idx < len(text):\n                    if text[text_idx] in foundation:\n                        x_chars.append(text[text_idx])\n                        positions_used.append(text_idx)\n                        text_idx += 1\n                        consumed += 1\n                    else:\n                        return None\n                \n                if consumed != cluster_size:\n                    return None\n                    \n                x_text = ''.join(x_chars)\n                \n            elif p_char == 'f':\n                # Match single final consonant\n                if text_idx < len(text) and text[text_idx] in foundation:\n                    f_text = text[text_idx]\n                    positions_used.append(text_idx)\n                    text_idx += 1\n                else:\n                    return None\n                    \n            else:\n                # Must match exact character\n                if text_idx < len(text) and text[text_idx] == p_char:\n                    positions_used.append(text_idx)\n                    text_idx += 1\n                else:\n                    return None\n            \n            pattern_idx += 1\n        \n        # Check if pattern fully matched\n        if pattern_idx == len(pattern):\n            return {\n                'pattern': pattern,\n                'x': x_text,\n                'f': f_text,\n                'positions': positions_used,\n                'start': min(positions_used),\n                'end': max(positions_used)\n            }\n        \n        return None\n    \n    def explore_segmentations(used_positions: Set[int], current_reading: List):\n        \"\"\"\n        Recursively explore all possible ways to segment remaining text.\n        \"\"\"\n        # If all positions used, we have a complete reading\n        if len(used_positions) == text_len:\n            all_readings.append(current_reading.copy())\n            return\n        \n        # Find next unused position\n        next_pos = 0\n        while next_pos < text_len and next_pos in used_positions:\n            next_pos += 1\n            \n        if next_pos >= text_len:\n            return\n        \n        # Try all patterns that could include this position\n        found_match = False\n        for pattern in vowel_patterns:\n            matches = find_pattern_matches_at_position(text, pattern)\n            \n            for match in matches:\n                # Check if this match includes our position and doesn't conflict\n                if next_pos in match['positions']:\n                    # Check for conflicts with already used positions\n                    if not any(pos in used_positions for pos in match['positions']):\n                        # Valid match - explore this path\n                        syllable = (match['x'] if match['x'] else None, \n                                  match['pattern'], \n                                  match['f'] if match['f'] else None)\n                        \n                        new_used = used_positions | set(match['positions'])\n                        new_reading = current_reading + [syllable]\n                        \n                        explore_segmentations(new_used, new_reading)\n                        found_match = True\n        \n        # If no pattern matched at this position, text might be incomplete\n        if not found_match:\n            # Could add handling for unmatched characters here\n            pass\n    \n    # Start exploration\n    explore_segmentations(set(), [])\n    \n    # Remove duplicates\n    unique_readings = []\n    seen = set()\n    for reading in all_readings:\n        reading_str = str(reading)\n        if reading_str not in seen:\n            seen.add(reading_str)\n            unique_readings.append(reading)\n    \n    return unique_readings\n\n# Use the new algorithm\nfind_canonical_reading_order = find_all_possible_readings",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:14:54.592113Z",
     "start_time": "2025-09-21T20:14:54.585785Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test the algorithm\n",
    "print(\"=\"*60)\n",
    "print(\"FINDING ALL POSSIBLE READING ORDERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_cases = [\n",
    "    (thai1, \"simple case\"),\n",
    "    (thai2, \"vowel before consonant\"),\n",
    "    (thai3, \"hidden vowel\"),\n",
    "    (thai4, \"AMBIGUOUS: ว could be final OR part of cluster\"),\n",
    "    (thaihard1, \"cluster case\"),\n",
    "    (thaihard2, \"อ as silent initial\"),\n",
    "    (thaihard3, \"อ with tone mark\"),\n",
    "    (thaihard4, \"อ in complex pattern\"),\n",
    "    (thaihard5, \"ไ before consonant\")\n",
    "]\n",
    "\n",
    "for thai_text, description in test_cases:\n",
    "    print(f\"\\n'{thai_text}' - {description}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    readings = find_canonical_reading_order(thai_text)\n",
    "\n",
    "    # Remove duplicate readings\n",
    "    unique_readings = []\n",
    "    seen = set()\n",
    "    for reading in readings:\n",
    "        reading_str = str(reading)\n",
    "        if reading_str not in seen:\n",
    "            seen.add(reading_str)\n",
    "            unique_readings.append(reading)\n",
    "\n",
    "    if len(unique_readings) == 0:\n",
    "        print(\"  No valid readings found!\")\n",
    "    elif len(unique_readings) == 1:\n",
    "        print(f\"  UNAMBIGUOUS - Single reading:\")\n",
    "        for initial, vowel, final in unique_readings[0]:\n",
    "            if final:\n",
    "                print(f\"    {initial} + {vowel} + {final}\")\n",
    "            else:\n",
    "                print(f\"    {initial} + {vowel}\")\n",
    "    else:\n",
    "        print(f\"  AMBIGUOUS - {len(unique_readings)} possible readings:\")\n",
    "        for i, reading in enumerate(unique_readings[:10]):  # Show first 10 readings\n",
    "            print(f\"  Reading {i+1}:\")\n",
    "            for initial, vowel, final in reading:\n",
    "                if final:\n",
    "                    print(f\"    {initial} + {vowel} + {final}\")\n",
    "                else:\n",
    "                    print(f\"    {initial} + {vowel}\")\n",
    "        if len(unique_readings) > 10:\n",
    "            print(f\"  ... and {len(unique_readings) - 10} more readings\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS OF AMBIGUITIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Analyze เลว specifically\n",
    "print(f\"\\nDetailed analysis of 'เลว':\")\n",
    "readings = find_canonical_reading_order(\"เลว\")\n",
    "unique_readings = []\n",
    "seen = set()\n",
    "for reading in readings:\n",
    "    reading_str = str(reading)\n",
    "    if reading_str not in seen:\n",
    "        seen.add(reading_str)\n",
    "        unique_readings.append(reading)\n",
    "\n",
    "if len(unique_readings) > 1:\n",
    "    print(f\"  Found {len(unique_readings)} interpretations:\")\n",
    "    for i, reading in enumerate(unique_readings):\n",
    "        print(f\"  Interpretation {i+1}:\")\n",
    "        for initial, vowel, final in reading:\n",
    "            # Explain what this means\n",
    "            if vowel == \"เxf\" and initial == \"ล\" and final == \"ว\":\n",
    "                print(f\"    {initial} + {vowel} + {final}  → ล is initial, ว is final consonant\")\n",
    "            elif vowel == \"เx\" and initial == \"ลว\" and not final:\n",
    "                print(f\"    {initial} + {vowel}  → ลว is a consonant cluster\")\n",
    "            else:\n",
    "                if final:\n",
    "                    print(f\"    {initial} + {vowel} + {final}\")\n",
    "                else:\n",
    "                    print(f\"    {initial} + {vowel}\")\n",
    "else:\n",
    "    print(\"  Only one interpretation found (should be ambiguous!)\")\n",
    "    print(\"  This suggests the algorithm needs improvement.\")"
   ],
   "id": "nx5rgxn06rm"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Debug: Check what patterns could match เลว\n",
    "print(\"Debugging เลว patterns:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_text = \"เลว\"\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Characters: {[c for c in test_text]}\")\n",
    "print()\n",
    "\n",
    "# Check which patterns could potentially match\n",
    "matching_patterns = []\n",
    "for pattern in vowel_patterns:\n",
    "    # Check if pattern contains the vowel marks in เลว\n",
    "    if \"เ\" in pattern:\n",
    "        # Could this pattern match?\n",
    "        if \"เx\" in pattern or \"เxf\" in pattern or \"เxว\" in pattern:\n",
    "            matching_patterns.append(pattern)\n",
    "            print(f\"Potential pattern: {pattern}\")\n",
    "\n",
    "print(f\"\\nFound {len(matching_patterns)} patterns with เ\")\n",
    "\n",
    "# Now manually test the two interpretations we expect\n",
    "print(\"\\n1. Testing เxf pattern (ล as x, ว as f):\")\n",
    "if \"เxf\" in vowel_patterns:\n",
    "    print(\"   Pattern เxf exists\")\n",
    "    # This should match as: เ[ล]f where ว is the final\n",
    "else:\n",
    "    print(\"   Pattern เxf NOT FOUND!\")\n",
    "\n",
    "print(\"\\n2. Testing เx pattern (ลว as cluster x):\")\n",
    "if \"เx\" in vowel_patterns:\n",
    "    print(\"   Pattern เx exists\")\n",
    "    # This should match as: เ[ลว] where ลว is a cluster\n",
    "else:\n",
    "    print(\"   Pattern เx NOT FOUND!\")\n",
    "\n",
    "print(\"\\n3. Testing เxว pattern (ล as x, ว part of vowel):\")\n",
    "if \"เxว\" in vowel_patterns:\n",
    "    print(\"   Pattern เxว exists\")\n",
    "    # This matches as: เ[ล]ว where ว is part of the vowel pattern\n",
    "else:\n",
    "    print(\"   Pattern เxว NOT FOUND!\")"
   ],
   "id": "wgi68qfpsfh"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test เลว specifically to see all interpretations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING เลว WITH NEW ALGORITHM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "text = \"เลว\"\n",
    "print(f\"\\nAnalyzing: {text}\")\n",
    "\n",
    "# Get all possible readings\n",
    "readings = find_canonical_reading_order(text)\n",
    "\n",
    "print(f\"Found {len(readings)} possible reading(s):\")\n",
    "\n",
    "for i, reading in enumerate(readings):\n",
    "    print(f\"\\nReading {i+1}:\")\n",
    "    for x, pattern, f in reading:\n",
    "        interpretation = []\n",
    "        if x:\n",
    "            interpretation.append(f\"initial={x}\")\n",
    "        interpretation.append(f\"pattern={pattern}\")\n",
    "        if f:\n",
    "            interpretation.append(f\"final={f}\")\n",
    "\n",
    "        # Explain what this means\n",
    "        if pattern == \"เxว\" and x == \"ล\":\n",
    "            print(f\"  {' + '.join(interpretation)}\")\n",
    "            print(f\"    → ล is initial, ว is part of vowel pattern เxว\")\n",
    "        elif pattern == \"เxf\" and x == \"ล\" and f == \"ว\":\n",
    "            print(f\"  {' + '.join(interpretation)}\")\n",
    "            print(f\"    → ล is initial, ว is final consonant\")\n",
    "        elif pattern == \"เx\" and x == \"ลว\":\n",
    "            print(f\"  {' + '.join(interpretation)}\")\n",
    "            print(f\"    → ลว is a consonant cluster\")\n",
    "        else:\n",
    "            print(f\"  {' + '.join(interpretation)}\")\n",
    "\n",
    "if len(readings) < 2:\n",
    "    print(\"\\n⚠️ PROBLEM: Should find multiple interpretations but only found\", len(readings))\n",
    "    print(\"Expected at least:\")\n",
    "    print(\"  1. ล + เxว (ว part of vowel)\")\n",
    "    print(\"  2. ล + เxf (ว as final)\")\n",
    "    print(\"  3. ลว + เx (cluster)\")\n",
    "else:\n",
    "    print(\"\\n✓ Successfully found ambiguity!\")"
   ],
   "id": "lhwmz88ozjp"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-21T20:14:54.607669Z",
     "start_time": "2025-09-21T20:14:54.606381Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c6accf4c8f55dc58",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
